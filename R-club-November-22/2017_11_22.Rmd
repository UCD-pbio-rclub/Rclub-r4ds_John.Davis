---
title: "2017_11_22"
author: "John D."
date: "November 22, 2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = T)
```

# 2. Statistical Learning
## 2.1 What is Statistical Learning?
### 2.1.1 Why Estimate f?
### 2.1.2 How Do We Estimate f?
### 2.1.3 The Trade-Off Between Prediction Accuraccy and Model Interpretability
### 2.1.4 Supervised Versus Unsupervised Learning
### 2.1.5 Regression Versus Classification Problems
## 2.2 Assessing Model Accuracy
### 2.2.1 Measuring the Quality of Fit
### 2.2.2 The Bias-Variance Trade-Off

# Problems

1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.


(a) The sample size n is extremely large, and the number of predictors p is small.

Would be better. Since we have a large sample size and few predictors a more flexible method may be able to better represent the population


(b) The number of predictors p is extremely large, and the number of observations n is small.

Would be worse. With such few observations and and many predictors the model may overfitted


(c) The relationship between the predictors and response is highly
non-linear.

Would be better. A more flexible model will most likely be needed to capture the non-linear relationship. A less flexible method would be to rigid.

(d) The variance of the error terms, i.e. Ïƒ2 = Var(), is extremely
high.

Would be worse. The high variance suggests the various flexible models are being overfit.

3. We now revisit the bias-variance decomposition

(a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis shouldrepresent the values for each curve. There should be five curves. Make sure to label each one.

Training error, test error, and Bayes (or irreducible) error curves Figure 2.10. Typical (squared) bias and variance Figure 2.12.


(b) Explain why each of the five curves has the shape displayed in part (a)

Training error decreases as flexibility increase because the model is being overfit to the training se.
Test error increases as flexibility increase because the model is being overfit to the training set.
Irreducible is a horizontal line that does change and averages 0.
Bias decreases as you get more flexible.
Variance increases because you begin overfititng to the training set and f() begins to vary more based off the training set.

4. You will now think of some real-life applications for statistical learning.

(a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer

Genotyping: The response is the genotype and the predictors are snps. Prediction

(b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

Phenotype prediction: The response is a predicted measurable trait and the predictors can be other traits and sequencing information. Predicition. Can also be infererence if you want to see which traits or sequences affect the desired trait

(c) Describe three real-life applications in which cluster analysis might be useful.

Clustering in diversity panel. Response is geographical regions and predictors can be traits. Inferring how samples are related.

6. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages?

In a parametric statistical learning approach we make an assumption of the functional form of f. We select the number of parameters and then estimate them. Can be much easier. The disadvantages are that we can pick a bad model and that more parameters can lead to overfitting.

In a non-parametric statistical learning approach we do not have to assume the form of f allowing for a wider range of possible shapes for f. A disadvantage is that they require a very large number of observations.