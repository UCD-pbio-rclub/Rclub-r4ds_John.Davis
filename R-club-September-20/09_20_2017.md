# 09_20_2017
John D.  
September 18, 2017  



# 22 Introduction

## 22.1 Hypothesis generation vs. hypothesis confirmation

# 23 Model basics

## 23.1 Introduction

### 23.1.1 Prerequisites


```r
library(tidyverse)
```

```
## Loading tidyverse: ggplot2
## Loading tidyverse: tibble
## Loading tidyverse: tidyr
## Loading tidyverse: readr
## Loading tidyverse: purrr
## Loading tidyverse: dplyr
```

```
## Conflicts with tidy packages ----------------------------------------------
```

```
## filter(): dplyr, stats
## lag():    dplyr, stats
```

```r
library(modelr)
options(na.action = na.warn)
```

## 23.2 A simple model


```r
ggplot(sim1, aes(x, y)) + 
  geom_point()
```

![](09_20_2017_files/figure-html/unnamed-chunk-2-1.png)<!-- -->

```r
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point() 
```

![](09_20_2017_files/figure-html/unnamed-chunk-2-2.png)<!-- -->

```r
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)
```

```
##  [1]  8.5  8.5  8.5 10.0 10.0 10.0 11.5 11.5 11.5 13.0 13.0 13.0 14.5 14.5
## [15] 14.5 16.0 16.0 16.0 17.5 17.5 17.5 19.0 19.0 19.0 20.5 20.5 20.5 22.0
## [29] 22.0 22.0
```

```r
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)
```

```
## [1] 2.665212
```

```r
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```

```
## # A tibble: 250 x 3
##             a1         a2      dist
##          <dbl>      <dbl>     <dbl>
##  1  -8.0478555  1.0001247 18.425610
##  2 -17.1046977  0.1922483 32.071038
##  3 -17.5864790 -1.6439189 43.500848
##  4  -9.2390615 -0.1628338 26.501616
##  5  19.3797466 -3.6233377 22.976247
##  6  23.6365332 -2.8088938 15.904661
##  7  -0.9610199 -3.3594011 38.301649
##  8 -14.0673132 -3.1545210 49.292492
##  9 -11.8158660  3.8075198  8.405744
## 10  -6.2393660  1.5128919 13.678159
## # ... with 240 more rows
```

```r
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```

![](09_20_2017_files/figure-html/unnamed-chunk-2-3.png)<!-- -->

```r
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```

![](09_20_2017_files/figure-html/unnamed-chunk-2-4.png)<!-- -->

```r
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```

![](09_20_2017_files/figure-html/unnamed-chunk-2-5.png)<!-- -->

```r
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```

![](09_20_2017_files/figure-html/unnamed-chunk-2-6.png)<!-- -->

```r
best <- optim(c(0, 0), measure_distance, data = sim1)
best$par
```

```
## [1] 4.222248 2.051204
```

```r
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

![](09_20_2017_files/figure-html/unnamed-chunk-2-7.png)<!-- -->

```r
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

```
## (Intercept)           x 
##    4.220822    2.051533
```

### 23.2.1 Exercises

1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?


```r
sim_models <- function(){
  sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

best <- optim(c(0, 0), measure_distance, data = sim1)
  
ggplot(sim1a, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
}

sim_models()
```

![](09_20_2017_files/figure-html/unnamed-chunk-3-1.png)<!-- -->

```r
sim_models()
```

![](09_20_2017_files/figure-html/unnamed-chunk-3-2.png)<!-- -->

```r
sim_models()
```

![](09_20_2017_files/figure-html/unnamed-chunk-3-3.png)<!-- -->

```r
sim_models()
```

![](09_20_2017_files/figure-html/unnamed-chunk-3-4.png)<!-- -->

The best model does not fit as nicely when you have unusual values. The model is forced to adapt to account for the unusual values. If they were not present, the model would line up more nicely with the rest of the data.

2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:


```r
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

measure_distance2 <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}

best <- optim(c(0, 0), measure_distance, data = sim1)
best$par
```

```
## [1] 4.222248 2.051204
```

```r
best <- optim(c(0, 0), measure_distance2, data = sim1)
best$par
```

```
## [1] 4.364852 2.048918
```

They are similar but not the same.

3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optima. What’s the problem with optimising a three parameter model like this?


```r
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```

There are unlimited combinations of a[1] and a[3] that would give you the same optimum y intercept.
